# # Otimização final:

# Otimização de hiperparâmetros do ambiente com objetivo 1 com normalização e 1.000.000 de passos por 200 trials
food_delivery_gym/FoodDelivery-medium-obj1-v0:
  n_timesteps: 18000000
  policy: 'MultiInputPolicy'
  n_envs: 4 
  learning_rate: 0.0009742009357947689
  ent_coef: 9.53697192932737e-07
  clip_range: 0.3
  n_steps: 256
  batch_size: 16
  n_epochs: 1
  gamma: 0.9912696235841435
  gae_lambda: 0.9936681407681269
  max_grad_norm: 1.92598340602166
  policy_kwargs: "dict(net_arch=dict(pi=[64], vf=[64]), activation_fn=nn.Tanh)"
  normalize: true

# Otimização de hiperparâmetros do ambiente com objetivo 2 com normalização e 1.000.000 de passos por 200 trials
food_delivery_gym/FoodDelivery-medium-obj2-v0:
  n_timesteps: 18000000
  policy: 'MultiInputPolicy'
  n_envs: 4 
  learning_rate: 1.3971195027500851e-05
  ent_coef: 2.3016065325155215e-05
  clip_range: 0.1
  n_steps: 1024
  batch_size: 8
  n_epochs: 20
  gamma: 0.9853901879025417
  gae_lambda: 0.975997042675116
  max_grad_norm: 1.362571608677646
  policy_kwargs: "dict(net_arch=dict(pi=[256, 256], vf=[256, 256]), activation_fn=nn.Tanh)"
  normalize: true

# Otimização de hiperparâmetros do ambiente com objetivo 3 com normalização e 1.000.000 de passos por 200 trials
food_delivery_gym/FoodDelivery-medium-obj3-v0:
  n_timesteps: 18000000
  policy: 'MultiInputPolicy'
  n_envs: 4 
  learning_rate: 0.0006906747318567277
  ent_coef: 0.018839296414662848
  clip_range: 0.2
  n_steps: 4096
  batch_size: 128
  n_epochs: 20
  gamma: 0.9984580058939658
  gae_lambda: 0.9948249364811154
  max_grad_norm: 1.9354851609206636
  policy_kwargs: "dict(net_arch=dict(pi=[64], vf=[64]), activation_fn=nn.Tanh)"
  normalize: true

# Otimização de hiperparâmetros do ambiente com objetivo 4 com normalização e 1.000.000 de passos por 200 trials
food_delivery_gym/FoodDelivery-medium-obj4-v0:
  n_timesteps: 18000000
  policy: 'MultiInputPolicy'
  n_envs: 4 
  learning_rate: 0.0005651168883682632
  ent_coef: 1.538964705609768e-08
  clip_range: 0.3 
  n_steps: 512
  batch_size: 4
  n_epochs: 1
  gamma: 0.9963026230261156
  gae_lambda: 0.9969240695734893
  max_grad_norm: 1.7113090560578745
  policy_kwargs: "dict(net_arch=dict(pi=[256, 256], vf=[256, 256]), activation_fn=nn.Tanh)"
  normalize: true

# Otimização de hiperparâmetros do ambiente com objetivo 7 com normalização e 1.000.000 de passos por 200 trials
food_delivery_gym/FoodDelivery-medium-obj7-v0:
  n_timesteps: 18000000
  policy: 'MultiInputPolicy'
  n_envs: 4 
  learning_rate: 0.0007940824817787931
  ent_coef: 0.03961324248165696
  clip_range: 0.4
  n_steps: 2048
  batch_size: 64
  n_epochs: 20
  gamma: 0.9965711620188762
  gae_lambda: 0.9972264993191691
  max_grad_norm: 0.3471134084468121
  policy_kwargs: "dict(net_arch=dict(pi=[64, 64], vf=[64, 64]), activation_fn=nn.Tanh)"
  normalize: true

# Otimização de hiperparâmetros do ambiente com objetivo 8 com normalização e 1.000.000 de passos por 200 trials
food_delivery_gym/FoodDelivery-medium-obj8-v0:
  n_timesteps: 18000000
  policy: 'MultiInputPolicy'
  n_envs: 4 
  learning_rate: 5.5135366820418394e-05
  ent_coef: 3.997929023060322e-07
  clip_range: 0.2
  n_steps: 512
  batch_size: 16
  n_epochs: 20
  gamma: 0.9871791349653122
  gae_lambda: 0.9981000510222722
  max_grad_norm: 1.9391147767625003
  policy_kwargs: "dict(net_arch=dict(pi=[64, 64], vf=[64, 64]), activation_fn=nn.Tanh)"
  normalize: true

# # Otimizações de Teste:

# # Otimização de hiperparâmetros do ambiente sem normalizar com 1.000.000 de passos por 50 trials
# food_delivery_gym/FoodDelivery-medium-obj1-v0:
#   n_timesteps: 10000000
#   policy: 'MultiInputPolicy'
#   n_envs: 4 
#   learning_rate: 8.110187140961812e-05
#   ent_coef: 6.611612993784828e-06
#   clip_range: 0.2
#   n_steps: 512
#   batch_size: 32
#   n_epochs: 20
#   gamma: 0.9983633683661667
#   gae_lambda: 0.9432249472372363
#   max_grad_norm: 1.9218758164203371
#   policy_kwargs: "dict(net_arch=dict(pi=[256, 256], vf=[256, 256]), activation_fn=nn.ReLU)"
#   normalize: false

# # Otimização de hiperparâmetros com 1.500.000 de passos por 50 trials
# food_delivery_gym/FoodDelivery-medium-obj1-v0:
#   n_timesteps: 18000000
#   policy: 'MultiInputPolicy'
#   n_envs: 4 
#   learning_rate: 0.0012385994626143365
#   ent_coef: 4.346389035215521e-07
#   clip_range: 0.2
#   n_steps: 512
#   batch_size: 32
#   n_epochs: 1
#   gamma: 0.995774212877831459
#   gae_lambda: 0.9230863694972054
#   max_grad_norm: 0.5730007621571269
#   policy_kwargs: "dict(net_arch=dict(pi=[64], vf=[64]), activation_fn=nn.Tanh)"
#   normalize: true